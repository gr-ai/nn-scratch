{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Function and Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(object):\n",
    "    \n",
    "    def __init__(self,hidden_layer_sizes):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.weights = []\n",
    "        self.n_layers = len(self.hidden_layer_sizes)\n",
    "        \n",
    "    #initialize weights and biases\n",
    "    def w_init(self,X, set_seed=0):\n",
    "        \n",
    "        w_init=[]\n",
    "        np.random.seed(set_seed)\n",
    "        \n",
    "        for i in range(self.n_layers+1):\n",
    "            n_neurons = self.hidden_layer_sizes[i] if i <  self.n_layers else 1 #will need to change if multiple classes\n",
    "            n_feat = X.shape[1]+1 if i==0 else self.hidden_layer_sizes[i-1]+1\n",
    "            w_init.append(np.array(0.1*np.random.rand(n_neurons,n_feat)))\n",
    "        self.weights = w_init\n",
    "    \n",
    "    #feedforward\n",
    "    def feed_forward(self,X):\n",
    "        \n",
    "        bias_units=np.ones(X.shape[0])\n",
    "        a = [X]\n",
    "                \n",
    "        for i in range( self.n_layers+1):\n",
    "            w = self.weights[i]\n",
    "            a[i]=np.column_stack((bias_units,a[i]))\n",
    "            z = np.inner(a[i],w)\n",
    "\n",
    "            a.append(sigmoid(z))\n",
    "        \n",
    "        return a\n",
    "\n",
    "    #backpropagation\n",
    "    def backprop(self, X, Y):\n",
    "        a = self.feed_forward(X)\n",
    "        n_data = X.shape[0]\n",
    "\n",
    "        D = [np.zeros_like(w) for w in self.weights]\n",
    "\n",
    "        for i, (x, y) in enumerate(zip(X, Y)):\n",
    "            delta = [[] for ind in range( self.n_layers + 2)]\n",
    "            grad_J = [[] for ind in range( self.n_layers+1)]\n",
    "\n",
    "\n",
    "            delta[-1] = a[-1][i]-Y[i]\n",
    "            grad_J[-1] = delta[-1] * a[-2][i]\n",
    "            D[-1] += grad_J[-1]/n_data\n",
    "            \n",
    "            for l in reversed(range(1, self.n_layers + 1)):\n",
    "                g_prime = a[l][i] * (1-a[l][i])\n",
    "                g_prime = g_prime.reshape(-1,1)\n",
    "                \n",
    "              \n",
    "                #this is here to skip over the bias units for layers other than the last one (which doesn't have a bias unit since it is the output)\n",
    "                if l ==  self.n_layers:\n",
    "                    delta[l] = np.dot(self.weights[l].transpose(), delta[l+1].reshape(-1,1)) * g_prime\n",
    "                else:\n",
    "                    delta[l] = np.dot(self.weights[l].transpose(), delta[l+1][1:]) * g_prime\n",
    "\n",
    "                grad_J[l-1] = delta[l][1:] * a[l-1][i]\n",
    "                D[l-1] += grad_J[l-1]/n_data\n",
    "\n",
    "        return D\n",
    "    \n",
    "    #train NN on data with learning rate lr\n",
    "    def train(self, X, Y, lr):\n",
    "        \n",
    "        #this could be improved by using an error tolerance rather than fixed number of iterations\n",
    "        for _ in range(1000):\n",
    "            D = self.backprop(X, Y)\n",
    "            delta_w = [-1.0*lr*d for d in D]\n",
    "            self.weights = [w + d for w,d in zip(self.weights, delta_w)]\n",
    "        \n",
    "    #numeric gradient check based on one sided finite difference to check backprop\n",
    "    def grad_check(self,X,Y):\n",
    "        X_temp=X\n",
    "        Y_temp=Y.reshape(-1,1)\n",
    "\n",
    "        numeric_grad=[np.zeros_like(w) for w in self.weights]\n",
    "\n",
    "        eps=0.001\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            for j in range(self.weights[i].shape[0]):\n",
    "                for k in range(self.weights[i].shape[1]):\n",
    "                    H0=self.feed_forward(X_temp)[-1]\n",
    "                    C0=nn_cost(H0,Y_temp)\n",
    "                    \n",
    "                    self.weights[i][j,k]+=eps\n",
    "                    \n",
    "                    H1=self.feed_forward(X_temp)[-1]\n",
    "                    C1=nn_cost(H1,Y_temp)\n",
    "                    \n",
    "                    self.weights[i][j,k]-=eps\n",
    "\n",
    "                    numeric_grad[i][j,k]=(C1-C0)/eps\n",
    "        return numeric_grad\n",
    "    \n",
    "    #NN cost function\n",
    "    def nn_cost(self, H, Y):\n",
    "        J=-1/H.shape[0]*np.sum(Y*np.log(H)+(1-Y)*np.log(1-H))\n",
    "        return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=pd.read_csv('Iris.csv')\n",
    "\n",
    "iris['Species']=iris['Species'].astype('category')\n",
    "\n",
    "X=iris.loc[:,['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n",
    "X=np.array(X)\n",
    "\n",
    "iris['virginica_vall']=iris['Species']=='Iris-virginica'\n",
    "iris['virginica_vall']=iris['virginica_vall'].astype(int)\n",
    "Y=iris['virginica_vall']\n",
    "Y=np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model Performance on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[97,  0],\n",
       "       [ 3, 50]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test against Iris dataset\n",
    "test=NeuralNet(np.array([2]))\n",
    "test.w_init(X)\n",
    "test.train(X,Y,0.1)\n",
    "confusion_matrix([round(t[0]) for t in test.feed_forward(X)[-1]],Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[99,  1],\n",
       "       [ 1, 49]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logistic regression for comparison\n",
    "log_reg=LogisticRegression(C=1E10, solver='lbfgs')\n",
    "pred=log_reg.fit(X,Y).predict(X)\n",
    "confusion_matrix(pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[97,  0],\n",
       "       [ 3, 50]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#skikit classifier for comparison. NOTE: performance depends on initial conditions\n",
    "mlp= MLPClassifier(hidden_layer_sizes=(2),solver= 'sgd', activation='relu', alpha=0, batch_size='auto', max_iter=10000)\n",
    "mlp_pred=mlp.fit(X,Y).predict(X)\n",
    "confusion_matrix(mlp_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50,  0,  0],\n",
       "       [ 0, 49,  1],\n",
       "       [ 0,  1, 49]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3, 3))#, random_state=1)\n",
    "Y_all=iris['Species']\n",
    "Y_all=np.array(Y_all)\n",
    "confusion_matrix(clf.fit(X,Y_all).predict(X),Y_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
